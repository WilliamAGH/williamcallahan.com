<!-- markdownlint-disable MD029 -->

# Batch Fetch Update Architecture

**Functionality:** `batch-fetch-update`

This document outlines the automated background refresh schedule and batch processing architecture for production data in the williamcallahan.com application.

## Overview

The application uses a cron-based scheduler (`scripts/scheduler.ts`) that runs continuously and triggers background data updates at optimized intervals. All scheduling is done in Pacific Time (America/Los_Angeles).

## Critical Issues & Bugs

### ‚úÖ FIXED: Blocking Scheduler Architecture (2025-06)

- **Previous Issue**: Used `spawnSync` which blocked entire scheduler if one job hung
- **Solution**:
  - Replaced all `spawnSync` calls with async `spawn`
  - Added concurrency protection with job tracking
  - Jobs now run independently without blocking
- **Impact**: Scheduler continues running even if individual jobs fail or hang

### ‚úÖ FIXED: Build-Time Logo Prefetch Optimization (2025-06-18)

- **Previous Issue**: `scripts/prefetch-data.ts` fetched 200+ logos during build, causing 10+ minute build times
- **Solution**: Created `scripts/prefetch-data-optimized.ts` that skips logo prefetching
- **Impact**:
  - Build times reduced from 10+ minutes to < 30 seconds
  - Logos now fetched on-demand at runtime with multi-tier caching
  - No user-facing performance impact due to caching
- **Fallback**: `npm run build:full` still available for complete prefetch if needed

### ‚úÖ FIXED: Script Consolidation (2025-06-19)

- **Previous Issue**: 9 duplicate scripts with overlapping functionality
- **Solution**: Created centralized `lib/server/data-fetch-manager.ts` with built-in CLI
- **Architecture**:
  - `DataFetchManager` class handles all data operations
  - `DataFetchManagerCLI` provides unified command-line interface
  - Single `scripts/data-updater.ts` entry point for all operations
  - Scheduler moved to `lib/server/scheduler.ts`
- **Benefits**:
  - Eliminated 90% of duplicate code (removed 6 scripts)
  - Single CLI supports all operations with flags
  - Consistent behavior and easier maintenance

### üü† HIGH Priority Issues

1. **Missing Type Definitions**
   - **Location**: `types/node-cron.d.ts`
   - **Issue**: Custom declaration instead of @types/node-cron
   - **Fix**: `npm install --save-dev @types/node-cron`

2. **Brittle Data Parsing**
   - **Location**: `scripts/prefetch-data.ts`
   - **Issue**: Regex parsing of TypeScript files breaks with formatting changes
   - **Fix**: Refactor data files to export pure data structures

3. **No Concurrency Protection**
   - **Issue**: No protection against overlapping jobs
   - **Impact**: Can cause data corruption or API rate limit issues
   - **Fix**: Implement job locking mechanism

4. **No Retry Mechanism**
   - **Issue**: Failed operations are not retried
   - **Impact**: Transient failures cause permanent data gaps
   - **Fix**: Implement dead letter queue

## Refresh Frequencies

### üìñ Bookmarks: Every 2 Hours (12x/day)

- **Schedule**: `0 */2 * * *` (at minute 0 of every 2nd hour)
- **Frequency**: 12 times per day
- **Rationale**: Bookmarks are actively consumed content that benefits from frequent updates
- **Times**: 12:00 AM, 2:00 AM, 4:00 AM, 6:00 AM, 8:00 AM, 10:00 AM, 12:00 PM, 2:00 PM, 4:00 PM, 6:00 PM, 8:00 PM, 10:00 PM

### üêô GitHub Activity: Daily (1x/day)

- **Schedule**: `0 0 * * *` (at midnight)
- **Frequency**: Once per day
- **Rationale**: GitHub contribution data changes daily and doesn't require more frequent updates

### üé® Logos: Weekly (1x/week)

- **Schedule**: `0 1 * * 0` (Sunday at 1:00 AM)
- **Frequency**: Once per week
- **Rationale**: Company logos rarely change, weekly refresh reduces API load while maintaining freshness

## Infrastructure Details

### Scheduler Architecture

```typescript
// scripts/scheduler.ts - FIXED: Now uses async spawn
const updateProcess = spawn("bun", ["run", "update-s3", "--", "--bookmarks"], {
  env: process.env,
  stdio: "inherit",
  detached: false,
});
// Jobs run independently without blocking scheduler
```

### Data Fetch Manager Architecture

```typescript
// lib/server/data-fetch-manager.ts
export class DataFetchManager {
  async fetchData(config: DataFetchConfig): Promise<DataFetchResult[]> {
    // Orchestrates all data fetching operations
    // Returns unified results for monitoring
  }
  
  async prefetchForBuild(): Promise<DataFetchResult[]> {
    // Optimized build-time fetch (S3 only)
  }
}

export class DataFetchManagerCLI {
  static async run(): Promise<void> {
    // Parses command-line arguments
    // Executes appropriate DataFetchManager methods
    // Handles process exit codes
  }
}
```

### Usage Examples

```bash
# Update all data
bun scripts/data-updater.ts

# Update specific data types
bun scripts/data-updater.ts --bookmarks
bun scripts/data-updater.ts --github-activity --logos

# Prefetch for builds
bun scripts/data-updater.ts --prefetch-build  # Fast (S3 only)
bun scripts/data-updater.ts --prefetch-dev    # Full (all sources)

# Force refresh
bun scripts/data-updater.ts --force --bookmarks
```

**Key Components:**

### Core Orchestrator

- **lib/server/data-fetch-manager.ts**: Centralized data fetching orchestrator
  - Handles bookmarks, GitHub activity, and logo fetching
  - Provides unified interface for all data operations
  - Manages batch processing, rate limiting, and retries

### Script Layer

- **lib/server/scheduler.ts**: Long-running process using node-cron
- **scripts/data-updater.ts**: Unified CLI for all data operations
  - `--bookmarks`: Update bookmarks only
  - `--github-activity`: Update GitHub activity only
  - `--logos`: Update logos only
  - `--prefetch-build`: Fast build prefetch (S3 only)
  - `--prefetch-dev`: Full development prefetch
  - `--force`: Force refresh regardless of cache
- **scripts/force-refresh-repo-stats.ts**: Manual GitHub stats refresh
- **scripts/refresh-opengraph-images.ts**: OpenGraph image backfilling

### Build-Time vs Runtime Data Strategy (2025-06-18)

**Build-Time Prefetch (via `prefetch-data-optimized.ts`):**

- ‚úÖ Bookmarks JSON from S3
- ‚úÖ GitHub activity data from S3
- ‚ùå Logos (skipped for faster builds)

**Runtime Fetching (2025-07 update ‚Äì streaming uploads):**

- Logos fetched on-demand when first requested
- Multi-tier caching ensures good performance:
  - Memory cache: ~1ms (ServerCacheInstance)
  - S3 cache: ~10-50ms (30-day TTL)
  - External API: 100ms-5s (only on cache miss)
  - **New:** logo downloads are now streamed directly from the source URL to S3. Memory footprint per logo is constant (~65 KB) because we no longer buffer the entire image in RAM before upload.

**Background Updates (via scheduler):**

- Bookmarks: Every 2 hours
- GitHub: Daily at midnight
- Logos: Weekly on Sundays (keeps S3 cache warm)

### Environment Configuration

All schedules can be overridden via environment variables:

```bash
# Bookmarks (default: every 2 hours)
S3_BOOKMARKS_CRON="0 */2 * * *"

# GitHub Activity (default: daily at midnight)
S3_GITHUB_CRON="0 0 * * *"

# Logos (default: weekly Sunday at 1 AM)
S3_LOGOS_CRON="0 1 * * 0"
```

### Staggered Execution

The schedules are deliberately staggered to prevent resource contention:

- **Bookmarks**: Every 2 hours at minute 0
- **GitHub**: Daily at midnight (00:00)
- **Logos**: Weekly Sunday at 1 AM (01:00)

## Authentication & Security

### Bookmarks

- Uses `BOOKMARK_CRON_REFRESH_SECRET` for authentication
- Supports both cron jobs (authenticated) and public API calls (rate-limited)
- Cron jobs bypass rate limiting and force refresh

### GitHub Activity

- Uses `GITHUB_REFRESH_SECRET` for authentication
- Protected endpoint requires valid secret header

### Logos

- No dedicated refresh endpoint (handled via update-s3 script)
- Uses existing logo fetching infrastructure with S3 caching

## Monitoring & Logging

### Scheduler Logs

```bash
[Scheduler] Process started. Setting up cron jobs...
[Scheduler] Bookmarks schedule: 0 */2 * * * (every 2 hours)
[Scheduler] GitHub Activity schedule: 0 0 * * * (daily at midnight)
[Scheduler] Logos schedule: 0 1 * * 0 (weekly Sunday 1 AM)
[Scheduler] Setup complete. Scheduler is running...
[Scheduler] Production frequencies: Bookmarks (12x/day), GitHub (1x/day), Logos (1x/week)
```

### Execution Logs

```bash
[Scheduler] [Bookmarks] Cron triggered at 2/15/2024, 2:00:00 AM. Spawning update-s3...
[Scheduler] [Bookmarks] update-s3 script completed successfully
```

### Error Handling Issues

**Current State:**

- Failed script executions are logged but not retried
- Scheduler can be blocked by hanging jobs
- No circuit breaker for external API failures
- Missing dead letter queue for failed operations

**Recommended Improvements:**

1. **Implement Job Queue with State Management**

```typescript
interface JobState {
  id: string;
  type: 'bookmarks' | 'github' | 'logos';
  status: 'pending' | 'running' | 'completed' | 'failed';
  startedAt?: Date;
  completedAt?: Date;
  error?: string;
  retryCount: number;
}
```

2. **Add Circuit Breaker Pattern**

```typescript
class CircuitBreaker {
  private failureCount = 0;
  private state: 'closed' | 'open' | 'half-open' = 'closed';
  
  async execute<T>(fn: () => Promise<T>): Promise<T> {
    if (this.state === 'open') {
      throw new Error('Circuit breaker is open');
    }
    // Implementation...
  }
}
```

3. **Implement Job Locking**

**‚ö†Ô∏è WARNING: The following example is for educational purposes only and demonstrates the concept. It is NOT safe for production multi-instance deployments due to race conditions (TOCTOU) and local filesystem dependencies.**

```typescript
// EDUCATIONAL EXAMPLE - Single Instance Only
const createLock = (jobName: string): boolean => {
  const lockFile = `/tmp/locks/${jobName}.lock`;
  if (existsSync(lockFile)) {
    console.warn(`Job ${jobName} is already running`);
    return false;
  }
  writeFileSync(lockFile, String(process.pid));
  return true;
};
```

**For production deployments**, use distributed locking mechanisms:

```typescript
// PRODUCTION EXAMPLE - Multi-Instance Safe
import Redis from 'ioredis';
import Redlock from 'redlock';

const redis = new Redis();
const redlock = new Redlock([redis]);

async function createDistributedLock(jobName: string): Promise<Lock | null> {
  try {
    // Lock for 5 minutes (300,000ms)
    const lock = await redlock.acquire([`locks:${jobName}`], 300000);
    return lock;
  } catch (error) {
    console.warn(`Job ${jobName} is already running on another instance`);
    return null;
  }
}

// Usage with automatic unlock on completion
async function runJobWithLock(jobName: string, job: () => Promise<void>) {
  const lock = await createDistributedLock(jobName);
  if (!lock) return;
  
  try {
    await job();
  } finally {
    await lock.release();
  }
}
```

See the [Scaling Considerations](#scaling-considerations) section for more distributed system patterns.

## API Endpoints

### Manual Refresh Endpoints

For manual triggering or debugging:

```bash
# Bookmarks refresh
curl -X POST http://localhost:3000/api/bookmarks/refresh \
  -H "Authorization: Bearer $BOOKMARK_CRON_REFRESH_SECRET"

# GitHub activity refresh
curl -X POST http://localhost:3000/api/github-activity/refresh \
  -H "x-refresh-secret: $GITHUB_REFRESH_SECRET"

# Logos (via update-s3 script)
bun run update-s3 -- --logos
```

### Status Check Endpoints

```bash
# Check bookmark cache status
curl http://localhost:3000/api/bookmarks/refresh

# No dedicated status endpoints for GitHub/Logos currently
```

## Performance Considerations

### Resource Usage

- **Bookmarks**: Moderate API load (12 calls/day to external services)
- **GitHub**: Low API load (1 call/day to GitHub GraphQL API)
- **Logos**: Very low API load (1 batch/week to logo providers)

### Caching Strategy

1. **In-Memory Cache**: Fast access for active requests
2. **S3 Storage**: Persistent storage between deployments
3. **External APIs**: Fallback when cache/S3 data is unavailable

### Rate Limiting

- Logo fetching includes configurable batch processing
- Built-in delays between batches to respect provider limits
- Environment configurable via `LOGO_BATCH_SIZE` and `LOGO_BATCH_DELAY_MS`

## Deployment & Operations

### Starting the Scheduler

```bash
# Production deployment
bun run scheduler

# Development
bun run scheduler:dev
```

### Health Checks

The scheduler process must remain running for automated updates. Monitor via:

- Process health (scheduler.ts running)
- Log output for successful cron triggers
- Data freshness in cache/S3

### Architectural Improvements Needed

1. **Replace Blocking Architecture**

```typescript
// WRONG: Current blocking approach
const result = spawnSync('bun', ['run', 'update-s3']);

// RIGHT: Non-blocking async approach
const updateProcess = spawn('bun', ['run', 'update-s3'], {
  env: process.env,
  stdio: 'inherit',
  detached: false
});

updateProcess.on('close', (code) => {
  if (code !== 0) {
    console.error(`Update failed with code ${code}`);
    // Add to dead letter queue
  }
});
```

2. **Implement Incremental Updates**

```typescript
interface SyncState {
  lastSyncAt: Date;
  lastSuccessfulSyncAt: Date;
  totalItemsSynced: number;
}

async function incrementalBookmarkSync(): Promise<void> {
  const syncState = await getSyncState('bookmarks');
  const newBookmarks = await fetchBookmarksSince(syncState.lastSuccessfulSyncAt);
  // Process only new/updated items
}
```

3. **Add Monitoring & Metrics**

```typescript
interface JobMetrics {
  jobType: string;
  duration: number;
  itemsProcessed: number;
  errors: number;
  timestamp: Date;
}
```

### Scaling Considerations

- Current implementation is single-instance and vulnerable to blocking
- For production deployments:
  - Use proper job queue (e.g., BullMQ, Agenda)
  - Implement distributed locking (Redis/DynamoDB)
  - Add health checks and monitoring
  - Consider serverless scheduled functions

## Troubleshooting

### Common Issues

1. **Scheduler not running**: Check if `scripts/scheduler.ts` process is alive
2. **Authentication failures**: Verify environment secrets are set
3. **External API failures**: Check rate limits and API availability
4. **S3 connection issues**: Verify AWS credentials and bucket access

### Debug Commands

```bash
# Check current cache status
curl http://localhost:3000/api/bookmarks/refresh

# Manual trigger for testing
bun run update-s3 -- --bookmarks --verbose

# View scheduler logs
tail -f /path/to/scheduler.log
```

This production schedule ensures fresh content while optimizing API usage and resource consumption.
